<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>
  <style>
    .navA{
      display: inline-block;
      margin-right: 13px;
      font-size: 16px;
      font-weight: 700;
      color: #000;
      text-decoration: none;
      padding: 5px ;
      border: #000 1px solid;
    }
    .navA:hover{
      color: #fff;
      background-color: #000;
    }
  </style>

  <title>Shaoyi Huang</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
<!--   <link rel="icon" type="image/png" href="images/seal_icon.png"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shaoyi Huang</name>
              </p>
              <p>Hello! I am a Ph.D. candidate in the Department of Computer Science and Engineering at <a href="https://www.cse.uconn.edu/">University of Connecticut</a> and advised by Prof. <a href="https://caiwending.cse.uconn.edu/">Caiwen Ding</a>. 
                Before joining University of Connecticut, I obtained a bachelor’s degree and master’s degree in Wuhan University of Technology and University of Rochester, respectively. 
              </p>
              <p> During 2019-2020, I was an algorithm engineer at <a href="https://www.huawei.com/en/">Huawei Technologies</a>, exploring intelligent alarm algorithms and developing artificial intelligent fault management framework.
                In summer 2022, I was a research intern in <a href="https://www.tiktok.com/en/">TikTok</a>, studying training acceleration on Transformer-based models.
              </p>
              <p style="text-align:center">
                <a href="data/ShaoyiHuang-email.txt">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Ybk2L10AAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/shaoyiHusky">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/shaoyi-huang-719ab611b/">LinkedIn</a>
              </p>
            </td>

            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ShaoyiHuang.jpg"><img style="width:60%;max-width:60%" alt="profile photo" src="images/ShaoyiHuang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <div class="navbar" style="padding-left: 18px;">
          <a href="#Research" class="navA">Research</a>
          <a href="#News" class="navA">News</a>
          <a href="#Publications" class="navA">Publications</a>
          <a href="#Services" class="navA">Services</a>
          <a href="#Teaching" class="navA">Teaching</a>
          <a href="#Honors" class="navA">Awards</a>
        </div>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
  
            <td width="100%" valign="middle">
  
              <heading id="Research"><b>Research</b></heading>
              <p>
              I'm interested in efficient and reliable machine learning.
              </p>
  
              <ul>
               <li><p>
                <strong>Algorithm-hardware (GPUs/FPGAs) Co-design for AI Acceleration and Green AI, Neuromorphic Computing</strong>
              </p>
                 
              <li><p>
                <strong>Efficient Training & Inference Algorithms</strong>
              </p>
              
               <li><p>
                <strong>Privacy-preserving Machine Learning</strong>
              </p>
              <li><p>
                <strong>Domains: Natural Language Processing, Computer Vision, Graph, Sciences</strong>
              </p>
  
              </ul>
            </td>
          </tr>

        <tr>
          <td>
            <heading id="News"><b>News</b></heading>
              <div class="list scroll">
              <ul>
                <li>04/2023: One paper is accepted to <b>IJCAI 2023</b>.</li>
                <li>03/2023: Two papers on sparse training are accepted to <b>DAC 2023</b>.</li>
                <li>01/2023: Invited to serve as a reviewer for the First Workshop on DL-Hardware Co-Design for AI Acceleration at AAAI 2023.</li>
                <li>12/2022: Invited to serve as a PC member for KDD 2023.</li>
                <li>08/2022: Received the <b>GE Fellowship for Excellence</b> from <a href="https://www.engr.uconn.edu/">the School of Engineering</a>. Thanks for the support!</li>
                <li>07/2022: Invited to serve as a PC member for AAAI 2023.</li>
                <li>07/2022: Received the <b>Synchrony Fellowship</b> from <a href="https://cacc.uconn.edu/">Connecticut Advanced Computing Center</a>. Thanks, CACC!</li>
                <li>05/2022: Started to work at <b>TikTok (ByteDance)</b> as a research intern.</li>
                <li>05/2022: I am very honored to receive the <b>Predoctoral Prize for Research Excellence</b> from <a href="https://www.cse.uconn.edu/">UConn CSE</a>.</li>
                <li>05/2022: Received the Eversource Energy Graduate Fellowship from <a href="https://www.eversource.uconn.edu/">Eversource Energy Center</a>. Thanks, Eversource!</li>
                <li>04/2022: Our <b>DAC 22</b> paper “A Length Adaptive Algorithm-Hardware Co-design of Transformer on FPGA Through Sparse Attention and Dynamic Pipelining” has been recognized as a <b>Publicity Paper</b>.</li>
                <li>02/2022: One paper is accepted to <b>ACL 2022</b>.</li>
                <li>02/2022: One paper is accepted to <b>DAC 2022</b>.</li>
                <li>02/2022: One paper is accepted to <b>ISQED 2022</b>.</li>
                <li>08/2021: Received the Cigna Graduate Fellowship from UConn CSE. Thanks, Cigna!</li>
                <li>07/2021: One paper is accepted to <b>ICCAD 2021</b>.</li>
                <li>06/2021: One paper is accepted to <b>SC 2021</b>.</li>
                <li>04/2021: Three papers are accepted to <b>GLSVLSI 2021</b>.</li>
                <li>02/2021: One paper is accepted to <b>ISQED 2021</b>.</li>
              </ul>
              </div>
          </td>
        </tr>
        </table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading id="Publications"><b>Publications</b></heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>2023</h2>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Shaoyi/peer_distillation.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Towards <font color="red">Lossless</font> Head Pruning through <font color="red">Automatic Peer Distillation</font> for Large Language Models</font></papertitle>
              </a>
              <br>
               B. Li, Z. Wang, <b>S. Huang</b>, M. Bragin, J. Li, C. Ding
              <br>
              <em><b>[IJCAI 2023]</b></em> <i>The 32nd International Joint Conference On Artificial Intelligence</i>
              <br>
<!--               <a href="https://arxiv.org/pdf/2211.16667.pdf">PDF</a> -->
              <p>we propose a counter-traditional pruning strategy, peer distillation, and reveal that <b>the knowledge learned in advance but discarded from the pruned 
                attention heads plays a crucial role in maintaining model accuracy</b>. Experimental results on nine GLUE benchmark tasks show that we achieve high
                compression rates with zero or minor accuracy degradation.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Shaoyi/DST-EE.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Dynamic Sparse Training via Balancing the <font color="red">Exploration-Exploitation Trade-off</font></papertitle>
              </a>
              <br>
              <b>S. Huang</b>, B. Lei, D. Xu, H. Peng, Y. Sun, M. Xie, C. Ding
              <br>
              <em><b>[DAC 2023]</b></em> <i>The 60th Design Automation Conference</i>
              <br>
              <a href="https://arxiv.org/pdf/2211.16667.pdf">PDF</a>
              <p>To assist explainable sparse training, we propose important weights exploitation and weights coverage exploration to characterize sparse training. Our method does not need to train dense models, achieving up to 95% sparsity ratio and even higher accuracy than dense training, with same amount of iterations.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Shaoyi/NDSNN.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><font color="red">Neurogenesis Dynamics-inspired</font> Spiking Neural Network Training Acceleration</papertitle>
              </a>
              <br>
              <b>S. Huang</b>, H. Fang, K. Mahmood, B. Lei, N. Xu, B. Lei, Y. Sun, D. Xu, W. Wen, C. Ding
              <br>
              <em><b>[DAC 2023]</b></em> <i>The 60th Design Automation Conference</i>
              <br>
              <p> We propose an energy efficient spiking neural network training workflow, and design a new drop-andgrow strategy with decreasing number of non-zero weights in the process of dynamically updating sparse mask. We demonstrate extremely high sparsity (i.e., 99%) model performance in SNN based vision tasks.</p>
            </td>
          </tr>	
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>2022</h2>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Shaoyi/SPD.png" alt="3DSP" width="160" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="http://www.personal.psu.edu/dux19/" id="3DSP"> -->
              <a id="3DSP">
                <papertitle><font color="red"> Sparse Progressive Distillation:</font> Resolving Overfitting under Pretrain-and-Finetune Paradigm</papertitle>
              </a>
              <br>
              <b>S. Huang*</b>, D. Xu*, I. E. Yen, S. Chang, B. Li, C. Ding, et al.
              <br>
              <em><b>[ACL 2022]</b></em> <i>The 60th Annual Meeting of the Association for Computational Linguistics</i>
              <br>
              <a href="https://arxiv.org/pdf/2110.08190.pdf">PDF</a> / <a href="https://github.com/shaoyiHusky/SparseProgressiveDistillation">Code</a> / <a href="http://www.personal.psu.edu/dux19/">Supp</a> / <a href="http://www.personal.psu.edu/dux19/">Slides</a>
              <p>We study <b>network pruning of Transformer-based language models</b> under the pre-training and fine-tuning paradigm and propose a <b>counter-traditional hypothesis</b> that pruning increases the risk of overfitting when performed during the fine-tuning phase.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Shaoyi/20222_BigData_MIA_NLP.png" alt="3DSP" width="160" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Analyzing and Defending against Membership Inference Attacks in Natural Language Processing Classification</papertitle>
              </a>
              <br>
              Y. Wang, N. Xu, <b>S. Huang</b>, K. Mahmood, D. Guo, C. Ding, W. Wen, S. Rajasekaran.
              <br>
              <em><b>[IEEE BigData 2022]</b></em> <i>IEEE International Conference on Big Data (Big Data)</i>
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10020711">PDF</a>
              <p>We propose a novel defense algorithm, Gap score Regularization Integrated Pruning (GRIP), 
                that is optimized by finding a sub-network from the original over-parameterized NLP model. 
                GRIP can prevent privacy leakage from MIA and achieve similar accuracy to the original NLP model. 
                As an additional side benefit, GRIP can also reduce the model storage and the computation overhead. I</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Shaoyi/length_adaptive.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>A Length Adaptive Algorithm-Hardware Co-design of Transformer on FPGA Through Sparse Attention and Dynamic Pipelining</font></papertitle>
              </a>
              <br>
              H. Peng*, <b>S. Huang*</b>, S. Chen, B. Li, T. Geng, A, Li, W. Jiang, W, Wen, J, Bi, H. Liu, C. Ding
              <br>
              <em><b>[DAC 2022]</b></em> <i>The 59th Design Automation Conference</i>
              <br>
              <a href="https://arxiv.org/pdf/2208.03646.pdf">PDF</a>
              <p>We propose a sequence length adaptive design to allocate coarse pipeline stages dynamically to eliminate pipeline bubbles and achieve the highest possible throughput under different sequence length inputs. Experiments show that our design has very small accuracy loss and has 80.2× and 2.6× speedup compared to CPU
              and GPU implementation, and 4 × higher energy efficiency than state-of-the-art GPU accelerator optimized via CUBLAS GEMM.</p>
            </td>
          </tr>
    
        
        </tbody></table>
  
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>2021</h2>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Shaoyi/ET.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>E.T.: Re-Thinking Self-Attention for Transformer Models on GPUs</font></papertitle>
              </a>
              <br>
              S. Chen*, <b>S. Huang*</b>, S. Pandey, B. Li, G. Gao, C. Ding, H. Liu
              <br>
              <em><b>[SC 2021]</b></em> <i>Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</i>
              <br>
              <a href="https://dl.acm.org/doi/pdf/10.1145/3458817.3476138">PDF</a>
              <p>We evaluate E.T. across a variety of benchmarks for Transformer, BERTBASE and DistilBERT, where E.T. presents superior performance over the mainstream projects, including the popular Nvidia
                Enterprise solutions, i.e., TensorRT and FasterTransformer.</p>
            </td>
          </tr>
  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Shaoyi/ICCAD_2021.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Accelerating Framework of Transformer by Hardware Design and Model Compression Co-Optimization</font></papertitle>
              </a>
              <br>
              P. Qi, E. Sha, Q. Zhuge, H. Peng, <b>S. Huang</b>, Z. Kong, Y. Song, B. Li
              <br>
              <em><b>[ICCAD 2021]</b></em> <i>2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)</i>
              <br>
              <a href="https://arxiv.org/pdf/2110.10030.pdf">PDF</a>
              <p>We design a accelerator that
                takes advantage of HP to solve the problem of concurrent random
                access. Experiments on Transformer and TinyBert model show
                that our framework can find different devices for various LC
                and AC, covering from low-end devices to high-end devices. Our
                HP can achieve higher sparsity ratio and is more flexible than
                other sparsity pattern. Our framework can achieve 37×, 1.9×,
                1.7× speedup compared to CPU,GPU and FPGA,respectively.</p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Shaoyi/ISQED_2021.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Accelerating transformer-based deep learning models on fpgas using column balanced block pruning</font></papertitle>
              </a>
              <br>
              H. Peng, <b>S. Huang</b>, T. Geng, A. Li, W. Jiang, H. Liu, W. Wang, C. Ding
              <br>
              <em><b>[ISQED 2021]</b></em> <i>2021 22nd International Symposium on Quality Electronic Design (ISQED)</i>
              <br>
              <a href="https://wangshusen.github.io/papers/ISQED2021.pdf">PDF</a>
              <p>We implement the Transformer model
                with proper hardware scheduling, and the experiments show that
                the Transformer inference on FPGA achieves 10.35 ms latency
                with the batch size of 32, which is 10.96 × speed up comparing to
                CPU platform and 2.08 × speed up comparing to GPU platform.</p>
            </td>
          </tr>
    
        </tbody></table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading id="Services">Professional Services</heading>

    <ul>
      <li>
        <b>Program Committee Member:</b>
          <ul>
            <li>
              KDD'23
            </li>
            <li>
              AAAI'23
            </li>
            <li>
              DCAA'23
            </li>
          </ul>
      </li>
    </ul>

    <ul>
      <li>
        <b>Journal Reviewer:</b>
          <ul>
            <li>
              Pattern Recognition
            </li>
            <li>
              Engineering Applications of Artificial Intelligence
            </li>
          </ul>
      </li>
    </ul>


        </td>
      </tr>
      </table>




    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading id="Teaching">Teaching Experiences</heading>

      <ul>
        <li>
          <b>Teaching Assistant at UConn</b>

              <ul>
                
              <li><p>
              CSE 4502/5717 - BigData Analytics, Spring 2023<br>
              Instructor: <a href="https://hesuining.weebly.com/">Prof. Suining He</a> &nbsp&nbsp
              </p>
                
              <li><p>
              CSE5819 - Introduction to Machine Learning, Fall 2022<br>
              Instructor: <a href="http://feimiao.org/index.html">Prof. Fei Miao</a> &nbsp&nbsp <!-- <br> -->
              </p>
              </ul>

        </li>
      </ul>

      <!-- <ul>
        <li>
          <b>Teaching Assistant at University of Rochester</b>
            <ul>
              <li><p>
                  CSC 791&591: Advanced Topics in Efficient Deep Learning<br>
                  Course Materials: <a href="https://d2l.ai/index.html">Dive into Deep Learning</a>
                  </p>
              </li>
            </ul>
        </li>
      </ul> -->

        </td>
      </tr>
      </table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading id="Honors">Honors and Awards</heading>


              <ul>
                <li>
                  GE Fellowship for Excellence, 2022
                </li>
                <li>
                  Synchrony Fellowship, 2022
                </li>
                <li>
                  Predoctoral Prize for Research Excellence, 2022
                </li>
                <li>
                  Eversource Energy Graduate Fellowship, 2022
                </li>
                <li>
                  Cigna Graduate Fellowship, 2021
                </li>
              </ul>


          </td>
        </tr>
        </table>







        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:0px">
              <br><hr width=80%>
              <p style="text-align:right;font-size:small;">
                *Last updated on 04/12/2023*
                <br>
                <a href="https://jonbarron.info/">This guy makes a nice webpage</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
