<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>
  <style>
    .navA{
      display: inline-block;
      margin-right: 13px;
      font-size: 16px;
      font-weight: 700;
      color: #000;
      text-decoration: none;
      padding: 5px ;
      border: #000 1px solid;
    }
    .navA:hover{
      color: #fff;
      background-color: #000;
    }
  </style>

  <title>Shaoyi Huang</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
<!--   <link rel="icon" type="image/png" href="images/seal_icon.png"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shaoyi Huang</name>
              </p>
              <p>Hello! I am a Ph.D. candidate in the Department of Computer Science and Engineering at <a href="https://www.cse.uconn.edu/">University of Connecticut</a> and advised by Prof. <a href="https://caiwending.cse.uconn.edu/">Caiwen Ding</a>. 
                Before joining University of Connecticut, I obtained a bachelor’s degree and master’s degree in Wuhan University of Technology and University of Rochester, respectively. 
              </p>
              <p> During 2018-2019, I was an algorithm engineer at <a href="https://www.huawei.com/en/">Huawei Technologies</a>, exploring intelligent alarm algorithms and developing artificial intelligent fault management framework.
                In summer 2022, I was a research intern in <a href="https://www.tiktok.com/en/">TikTok</a>, studying training acceleration on Transformer-based models.
              </p>
              <p style="text-align:center">
                <a href="data/ShaoyiHuang-email.txt">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Ybk2L10AAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/shaoyiHusky">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/shaoyi-huang-719ab611b/">LinkedIn</a>
              </p>
            </td>

            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ShaoyiHuang.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/ShaoyiHuang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <div class="navbar" style="padding-left: 18px;">
          <a href="#Research" class="navA">Research</a>
          <a href="#News" class="navA">News</a>
          <a href="#Publications" class="navA">Publications</a>
          <a href="#Teaching" class="navA">Teaching</a>
          <a href="#Services" class="navA">Services</a>
          <a href="#Honors" class="navA">Awards</a>
        </div>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
  
            <td width="100%" valign="middle">
  
              <heading id="Research"><b>Research</b></heading>
              <p>
              I'm interested in efficient and reliable machine learning.
              </p>
  
              <ul>
              <li><p>
                <strong>Efficient Training & Inference Algorithms</strong>
              </p>
              <li><p>
                <strong>Algorithm-hardware Co-design for AI Acceleration and Green AI</strong>
              </p>
               <li><p>
                <strong>Privacy-preserving Machine Learning</strong>
              </p>
              <li><p>
                <strong>Domains: Natural Language Processing, Computer Vision, Graph, Sciences</strong>
              </p>
  
              </ul>
            </td>
          </tr>

        <tr>
          <td>
            <heading id="News"><b>News</b></heading>
              <div class="list scroll">
              <ul>
                <li>03/2023: Two papers on sparse training are accepted to <b>DAC 2023</b>.</li>
                <li>01/2023: Invited to serve as a reviewer for the First Workshop on DL-Hardware Co-Design for AI Acceleration at AAAI 2023.</li>
                <li>12/2022: Invited to serve as a PC member for KDD 2023.</li>
                <li>08/2022: Received the <b>GE Fellowship for Excellence</b> from <a href="https://www.engr.uconn.edu/">the School of Engineering</a>. Thanks for the support!</li>
                <li>07/2022: Invited to serve as a PC member for AAAI 2023.</li>
                <li>07/2022: Received the <b>Synchrony Fellowship</b> from <a href="https://cacc.uconn.edu/">Connecticut Advanced Computing Center</a>. Thanks, CACC!</li>
                <li>05/2022: Started to work at <b>Tiktok (ByteDance)</b> as a research intern.</li>
                <li>05/2022: I am very honored to receive the <b>Predoctoral Prize for Research Excellence</b> from <a href="https://www.cse.uconn.edu/">UConn CSE</a>.</li>
                <li>05/2022: Received the Eversource Energy Graduate Fellowship from <a href="https://www.eversource.uconn.edu/">Eversource Energy Center</a>. Thanks, Eversource!</li>
                <li>04/2022: Our <b>DAC 22</b> paper “A Length Adaptive Algorithm-Hardware Co-design of Transformer on FPGA Through Sparse Attention and Dynamic Pipelining” has been recognized as a <b>Publicity Paper</b>.</li>
                <li>02/2022: One paper was accepted to <b>ACL 2022 main conference</b>.</li>
                <li>02/2022: One paper was accepted to <b>DAC 2022</b>.</li>
                <li>02/2022: One paper was accepted to <b>ISQED 2022</b>.</li>
                <li>08/2021: Received the Cigna Graduate Fellowship from UConn CSE. Thanks, Cigna!</li>
                <li>07/2021: One paper was accepted to <b>ICCAD 2021</b>.</li>
                <li>06/2021: One paper was accepted to <b>SC 2021</b>.</li>
                <li>04/2021: Three papers were accepted to <b>GLSVLSI 2021</b>.</li>
                <li>02/2021: One paper was accepted to <b>ISQED 2021</b>.</li>
              </ul>
              </div>
          </td>
        </tr>
        </table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading id="Publications"><b>Publications</b></heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>2023</h2>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Shaoyi/DST-EE.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Dynamic Sparse Training via Balancing the <font color="red">Exploration-Exploitation Trade-off</font></papertitle>
              </a>
              <br>
              <b>S. Huang</b>, B. Lei, D. Xu, H. Peng, Y. Sun, M. Xie, C. Ding
              <br>
              <em><b>[DAC 2023]</b></em> <i>The 60th Design Automation Conference</i>
              <br>
              <a href="https://arxiv.org/pdf/2211.16667.pdf">PDF</a>
              <p>To assist explainable sparse training, we propose important weights exploitation and weights coverage exploration to characterize sparse training. Our method does not need to train dense models, achieving up to 95% sparsity ratio and even higher accuracy than dense training, with same amount of iterations.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Shaoyi/NDSNN.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><font color="red">Neurogenesis Dynamics-inspired</font> Spiking Neural Network Training Acceleration</papertitle>
              </a>
              <br>
              <b>S. Huang</b>, H. Fang, K. Mahmood, B. Lei, N. Xu, B. Lei, Y. Sun, D. Xu, W. Wen, C. Ding
              <br>
              <em><b>[DAC 2023]</b></em> <i>The 60th Design Automation Conference</i>
              <br>
              <p> We propose an energy efficient spiking neural network training workflow, and design a new drop-andgrow strategy with decreasing number of non-zero weights in the process of dynamically updating sparse mask. We demonstrate extremely high sparsity (i.e., 99%) model performance in SNN based vision tasks.</p>
            </td>
          </tr>	
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>2022</h2>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Shaoyi/length_adaptive.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>A Length Adaptive Algorithm-Hardware Co-design of Transformer on FPGA Through Sparse Attention and Dynamic Pipelining</font></papertitle>
              </a>
              <br>
              H. Peng*, <b>S. Huang*</b>, S. Chen, B. Li, T. Geng, A, Li, W. Jiang, W, Wen, J, Bi, H. Liu, C. Ding
              <br>
              <em><b>[DAC 2022]</b></em> <i>The 59th Design Automation Conference</i>
              <br>
              <a href="https://arxiv.org/pdf/2208.03646.pdf">PDF</a>
              <p>We propose a sequence length adaptive design to allocate coarse pipeline stages dynamically to eliminate pipeline bubbles and achieve the highest possible throughput under different sequence length inputs. Experiments show that our design has very small accuracy loss and has 80.2× and 2.6× speedup compared to CPU
              and GPU implementation, and 4 × higher energy efficiency than
               state-of-the-art GPU accelerator optimized via CUBLAS GEMM</p>
            </td>
          </tr>

<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Shaoyi/NDSNN.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><font color="red">Neurogenesis Dynamics-inspired</font> Spiking Neural Network Training Acceleration</papertitle>
              </a>
              <br>
              <b>S. Huang</b>, H. Fang, K. Mahmood, B. Lei, N. Xu, B. Lei, Y. Sun, D. Xu, W. Wen, C. Ding
              <br>
              <em><b>[DAC 2023]</b></em> <i>The 60th Design Automation Conference</i>
              <br>
              <p> We propose an energy efficient spiking neural network training workflow, and design a new drop-andgrow strategy with decreasing number of non-zero weights in the process of dynamically updating sparse mask. We demonstrate extremely high sparsity (i.e., 99%) model performance in SNN based vision tasks.</p>
            </td>
          </tr>	 -->
        </tbody></table>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:0px">
              <br><hr width=80%>
              <p style="text-align:right;font-size:small;">
                *Last updated on 04/12/2023*
                <br>
                <a href="https://jonbarron.info/">This guy makes a nice webpage</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
