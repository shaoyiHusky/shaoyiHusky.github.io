<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shaoyi Huang</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/seal_icon.png"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shaoyi Huang</name>
              </p>
              <p>Hello! I am a Ph.D. candidate in the Department of Computer Science and Engineering at <a href="https://www.cse.uconn.edu/">University of Connecticut</a> and advised by Prof. <a href="https://caiwending.cse.uconn.edu/">Caiwen Ding</a>. 
                Before joining University of Connecticut, I obtained a bachelor’s degree and master’s degree in Wuhan University of Technology and University of Rochester, respectively. 
              </p>
              <p> During 2018-2019, I was an algorithm engineer at <a href="https://www.huawei.com/en/">Huawei Technologies</a>, exploring intelligent alarm algorithms and developing artificial intelligent fault management framework.
                In summer 2022, I was a research intern in <a href="https://www.tiktok.com/en/">TikTok</a>, studying training acceleration on Transformer-based models.
              </p>
<!--               <p>I am a Ph.D candidate advised by Prof. <a href="https://caiwending.cse.uconn.edu/">Caiwen Ding</a> at <a href="https://www.cse.uconn.edu/">University of Connecticut</a>, where I work on machine learning, natural language processing.
                My research interests are Efficient AI, algorithm and hardware co-design. I received my M.S. degree from university of Rochester in 2018.
              </p> -->

              <!-- <p>
                Before I came to <a href="https://caiwending.cse.uconn.edu/">Uconn</a>, I spent more than 1 year in <a href="https://en.wikipedia.org/wiki/Huawei">Huawei Technologies</a> as a full-time algorithm engineer.
              </p> -->

              <p style="text-align:center">
                <a href="data/ShaoyiHuang-email.txt">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Ybk2L10AAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/shaoyiHusky">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/shaoyi-huang-719ab611b/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ShaoyiHuang.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/ShaoyiHuang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <div class="navbar" style="padding-left: 18px;">
          <a href="#Research" class="navA">Research</a>
          <a href="#News" class="navA">News</a>
          <a href="#Publications" class="navA">Publications</a>
          <a href="#Teaching" class="navA">Teaching</a>
          <a href="#Services" class="navA">Services</a>
          <a href="#Honors" class="navA">Awards</a>
        </div>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td>
              <heading>News</heading>
                <ul>
                  <!-- <li>10/2021: </li> -->
                  <li>03/2023: Two papers are accepted by <b>DAC 2023</b>.</li>
                  <li>01/2023: I will serve as a reviewer for the First Workshop on DL-Hardware Co-Design for AI Acceleration at AAAI 2023.</li>
                  <li>12/2022: I will serve as PC for KDD 2023.</li>
                  <li>08/2022: Received the <b>GE Fellowship for Excellence</b> from <a href="https://www.engr.uconn.edu/">the School of Engineering</a>. Thanks for the support!</li>
                  <li>07/2022: I will serve as PC for AAAI 2023.</li>
                  <li>07/2022: Received the <b>Synchrony Fellowship</b> from <a href="https://cacc.uconn.edu/">Connecticut Advanced Computing Center</a>. Thanks, CACC!</li>
                  <li>05/2022: Excited to join <b>Tiktok (ByteDance)</b> as a research intern!</li>
                  <li>05/2022: I am very honored to receive the <b>Predoctoral Prize for Research Excellence</b> from <a href="https://www.cse.uconn.edu/">UConn CSE</a>.</li>
                  <li>05/2022: Received the Eversource Energy Graduate Fellowship from <a href="https://www.eversource.uconn.edu/">Eversource Energy Center</a>. Thanks, Eversource!</li>
                  <li>04/2022: Our <b>DAC 22</b> paper “A Length Adaptive Algorithm-Hardware Co-design of Transformer on FPGA Through Sparse Attention and Dynamic Pipelining” has been recognized as a <b>Publicity Paper</b>.</li>
                  <li>02/2022: One paper “Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm” is accepted by <b>ACL 2022 main conference</b>.</li>
                  <li>02/2022: One paper "A Length Adaptive Algorithm-Hardware Co-design of Transformer on FPGA Through Sparse Attention and Dynamic Pipelining" is accepted by <b>DAC 2022</b>.</li>
                  <li>02/2022: One paper is accepted by <b>ISQED 2022</b>.</li>
                  <li>08/2021: Received the Cigna Graduate Fellowship from UConn CSE. Thanks, Cigna!</li>
                  <li>07/2021: One paper "Accelerating Framework of Transformer by Hardware Design and Model Compression Co-Optimization" is accepted by <b>ICCAD 2021</b>.</li>
                  <li>06/2021: One paper "Re-Thinking Self-Attention for Transformer Models on GPUs" is accepted by <b>SC 2021</b>.</li>
                  <li>04/2021: Three papers are accepted by <b>GLSVLSI 2021</b>.</li>
                  <li>02/2021: One paper is accepted by <b>ISQED 2021</b>.</li>
                </ul>
            </td>
          </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        	
            <tr>
              <td style="padding:0px">
                <br><hr width=80%>
                <p style="text-align:right;font-size:small;">
                  <!-- *Last updated on 02/01/2022* -->
                  <br>
                  Webpage template <a href="https://jonbarron.info/">source code</a>.
                </p>
              </td>
            </tr>
          </tbody></table>


      </td>
    </tr>
  </table>
</body>

</html>
